(LSTM層の入力データ)

・データを時系列方向に流す

１・入力データ=(バッチサイズ,シーケンス長,特徴量の次元)

例：
・株価データ(時系列)
　・バッチサイズ=32(1回の学習で処理するデータ数)
　・シーケンス長=10(過去10日分のデータ)
　・特徴量の次元=1(1つの株価値)
　・データの形状：(32,10,1)
・文章データ(NLP)
　・バッチサイズ=64(1回の学習で処理する文章の数)
　・シーケンス長=20(1文あたり20単語)
　・特徴量の次元=100(埋め込みベクトルの次元)
　・データの形状：(64,20,100)

２．LSTM層の処理

・各時刻のデータを入力し、前の時刻の情報を考慮しながら処理する。

ステップ１．忘却ゲート

    現在の時刻の入力層の入力と前の時刻の隠れ状態を組み合わせ、重みを掛け合わせ、シグモイド関数でデータを処理

ステップ2.入力ゲート

    １：入力ゲートの計算(新しい情報をどれくらい記憶するかを決める)

    １：前の時刻の隠れ状態と現在の入力をまとめる
    ２：前の時刻の隠れ状態と現在の入力を重みを使って変換
    ３：バイアスを加える
    ４：シグモイド関数で情報をどれくらい記憶するかを決める

ステップ3.候補記憶セルの計算(新しく記憶するための情報を生成)

    １：前の時刻の隠れ状態と現在の入力をまとめる
    ２：前の時刻の隠れ状態と現在の入力を重みを使って変換
    ３：バイアスを加える
    ４：双曲線正接関数（tanh）を適用
        出力の値を 
            −1 〜 1 の範囲に収める

ステップ４．記憶セルの更新(記憶セル（Cell State）をどのように更新するか)

    忘却ゲートの値×前の記憶セルの状態＋入力ゲートの値×候補記憶セル

ステップ５．出力ゲート

隠れ状態　→　シグモイド関数(出力ゲートの重み×直前の隠れ状態と現在の入力を組み合わせた値＋バイアス)　×　tanh(現在の記憶セルの値)

ステップ５．次の時刻の隠れ層に出力
隠れ状態
記憶セル

スッテップ６．最終時刻の隠れ層をタスクに応じた層に繋げる。

